# -*- coding: utf-8 -*-
"""UIUC+OSU+WFU.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1PjYECmhyXv5ESBwSPHswDXpkJdAaPNbA

UIUC
"""

import os, re, csv, asyncio
from urllib.parse import urlparse
from playwright.async_api import async_playwright

START_URL = "https://grainger.illinois.edu/about/directory/faculty"

OUT_DIR  = "/content/gdrive/MyDrive/prof_photos/uiuc_grainger_faculty/photos"
CSV_PATH = "/content/gdrive/MyDrive/prof_photos/uiuc_grainger_faculty/uiuc_grainger_faculty.csv"

os.makedirs(OUT_DIR, exist_ok=True)
os.makedirs(os.path.dirname(CSV_PATH), exist_ok=True)

BAD_KEYWORDS = ["logo", "icon", "sprite", "banner", "placeholder", "default", "loading", "spinner", "skip"]
BAD_ALT_WORDS = ["logo", "icon", "banner", "placeholder", "default", "skip"]

def safe_filename(name: str) -> str:
    name = (name or "").strip()
    name = re.sub(r"\s+", " ", name)
    name = re.sub(r"[^A-Za-z0-9 .,_-]+", "", name).strip()
    name = name.replace(" ", "_")
    return name[:180] if name else "unknown"

def ext_from_url(u: str) -> str:
    ext = os.path.splitext(urlparse(u).path)[1].lower()
    return ext if ext in [".jpg", ".jpeg", ".png", ".webp"] else ".jpg"

def ext_from_content_type(ct: str) -> str:
    ct = (ct or "").lower()
    if "png" in ct: return ".png"
    if "webp" in ct: return ".webp"
    if "jpeg" in ct or "jpg" in ct: return ".jpg"
    return ".jpg"

def unique_path(folder: str, base: str, ext: str) -> str:
    p = os.path.join(folder, base + ext)
    if not os.path.exists(p):
        return p
    k = 2
    while True:
        p2 = os.path.join(folder, f"{base}_{k}{ext}")
        if not os.path.exists(p2):
            return p2
        k += 1

def is_bad_image(url: str, alt: str = "") -> bool:
    low = (url or "").lower()
    a = (alt or "").lower()
    if not low: return True
    if low.startswith("data:image"): return True
    if any(k in low for k in BAD_KEYWORDS): return True
    if any(k in a for k in BAD_ALT_WORDS): return True
    return False

async def scrape_uiuc_grainger_faculty():
    async with async_playwright() as p:
        browser = await p.chromium.launch(
            headless=True,
            args=["--no-sandbox", "--disable-setuid-sandbox", "--disable-dev-shm-usage"],
        )
        context = await browser.new_context(
            user_agent=(
                "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 "
                "(KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36"
            )
        )
        page = await context.new_page()
        await page.goto(START_URL, wait_until="domcontentloaded", timeout=60000)
        await page.wait_for_timeout(1500)

        # Some directories lazy-load; scroll a few times to be safe.
        for _ in range(12):
            await page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
            await page.wait_for_timeout(900)

        # Extract records from the directory page.
        records = await page.evaluate("""
        () => {
          const base = location.href;
          const abs = (u) => { try { return new URL(u, base).href; } catch(e) { return null; } };

          function getImgUrl(img){
            let u = img.currentSrc || img.src || "";
            if (!u) {
              const srcset = img.getAttribute("srcset") || "";
              if (srcset) {
                const parts = srcset.split(",").map(s => s.trim().split(" ")[0]).filter(Boolean);
                if (parts.length) u = parts[parts.length - 1];
              }
            }
            return u ? abs(u) : null;
          }

          function looksLikeName(t){
            if (!t) return false;
            t = t.trim();
            if (!/[A-Za-z]/.test(t)) return false;
            const words = t.split(/\\s+/);
            return words.length >= 2 && t.length >= 4;
          }

          // Strategy:
          // - Find candidate "name links" that look like person names.
          // - Use a nearby container as the card, and select the largest img inside it.
          const links = Array.from(document.querySelectorAll("a[href]"))
            .map(a => ({a, text: (a.textContent||"").trim(), href: a.getAttribute("href")||""}))
            .filter(x => x.text && looksLikeName(x.text));

          const out = [];

          for (const item of links) {
            const profile_url = abs(item.href);
            if (!profile_url) continue;

            const card = item.a.closest("article, li, section, div") || item.a.parentElement;
            if (!card) continue;

            const imgs = Array.from(card.querySelectorAll("img"));
            let chosen = null;

            if (imgs.length) {
              // Pick the largest image in the card.
              chosen = imgs
                .map(img => ({img, w: img.naturalWidth||0, h: img.naturalHeight||0, alt: img.alt||""}))
                .sort((a,b) => (b.w*b.h) - (a.w*a.h))[0].img;
            }

            const image_url = chosen ? getImgUrl(chosen) : null;
            const image_alt = chosen ? (chosen.alt || "") : "";

            out.push({ name: item.text, profile_url, image_url, image_alt });
          }

          // Deduplicate by profile_url
          const map = new Map();
          for (const r of out) {
            if (!r.profile_url) continue;
            if (!map.has(r.profile_url)) map.set(r.profile_url, r);
            else {
              const cur = map.get(r.profile_url);
              // Prefer record with an image_url
              if (!cur.image_url && r.image_url) map.set(r.profile_url, r);
            }
          }

          return Array.from(map.values());
        }
        """)

        # Dedup by name+profile_url (extra safety)
        seen = set()
        uniq = []
        for r in records:
            key = ((r.get("name") or "").strip().lower(), (r.get("profile_url") or "").strip().lower())
            if key in seen:
                continue
            seen.add(key)
            uniq.append(r)

        print("Records found on directory page:", len(uniq))
        if len(uniq) == 0:
            await context.close()
            await browser.close()
            raise RuntimeError("No records found. The page structure may have changed.")

        # Fallback: if directory image is missing/bad, open profile page to find a better portrait.
        detail_page = await context.new_page()

        rows = []
        saved = 0

        for i, r in enumerate(uniq, 1):
            name = (r.get("name") or f"unknown_{i}").strip()
            profile_url = r.get("profile_url")
            image_url = r.get("image_url")
            image_alt = r.get("image_alt") or ""

            if (not image_url) or is_bad_image(image_url, image_alt):
                try:
                    await detail_page.goto(profile_url, wait_until="domcontentloaded", timeout=60000)
                    await detail_page.wait_for_timeout(800)

                    rec2 = await detail_page.evaluate("""
                    () => {
                      const base = location.href;
                      const abs = (u) => { try { return new URL(u, base).href; } catch(e) { return null; } };

                      // Try og:image first
                      const og = document.querySelector("meta[property='og:image']");
                      if (og && og.getAttribute("content")) {
                        return { image_url: abs(og.getAttribute("content")), image_alt: "" };
                      }

                      // Otherwise pick the largest image in main/article content
                      const imgs = Array.from(document.querySelectorAll("main img, article img, img"));
                      if (!imgs.length) return { image_url: null, image_alt: "" };

                      const best = imgs
                        .map(img => ({img, w: img.naturalWidth||0, h: img.naturalHeight||0}))
                        .sort((a,b) => (b.w*b.h) - (a.w*a.h))[0].img;

                      let u = best.currentSrc || best.src || "";
                      if (!u) {
                        const srcset = best.getAttribute("srcset") || "";
                        if (srcset) {
                          const parts = srcset.split(",").map(s => s.trim().split(" ")[0]).filter(Boolean);
                          if (parts.length) u = parts[parts.length - 1];
                        }
                      }
                      return { image_url: u ? abs(u) : null, image_alt: best.alt || "" };
                    }
                    """)
                    if rec2 and rec2.get("image_url"):
                        image_url = rec2["image_url"]
                        image_alt = rec2.get("image_alt", "")
                except Exception:
                    pass

            local_path = ""
            if image_url and (not is_bad_image(image_url, image_alt)):
                try:
                    resp = await context.request.get(image_url, timeout=60000)
                    if resp.ok:
                        ct = (resp.headers.get("content-type") or "").lower()
                        if "image" in ct:
                            ext = ext_from_content_type(ct) or ext_from_url(image_url)
                            base = safe_filename(name)
                            path = unique_path(OUT_DIR, base, ext)
                            data = await resp.body()
                            with open(path, "wb") as f:
                                f.write(data)
                            local_path = path
                            saved += 1
                except Exception:
                    local_path = ""

            rows.append({
                "name": name,
                "profile_url": profile_url,
                "image_url": image_url,
                "local_path": local_path
            })

        with open(CSV_PATH, "w", newline="", encoding="utf-8-sig") as f:
            w = csv.DictWriter(f, fieldnames=["name", "profile_url", "image_url", "local_path"])
            w.writeheader()
            w.writerows(rows)

        await context.close()
        await browser.close()

        print("Total rows:", len(rows))
        print("Images saved:", saved)
        print("CSV saved to:", CSV_PATH)
        print("Images folder:", OUT_DIR)

        return rows

rows = await scrape_uiuc_grainger_faculty()
rows[:5]

"""OSU"""

import os, re, time
from urllib.parse import urljoin, urlparse
import requests
import pandas as pd
from bs4 import BeautifulSoup
from tqdm import tqdm

from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry

# ====== Config ======
LIST_URL = "https://engineering.osu.edu/directory?field_departments_target_id=All&classification=14"
BASE = "https://engineering.osu.edu"
PLACEHOLDER = "https://engineering.osu.edu/themes/custom/osu_kinetic/images/profile-placeholder.png"

OUT_DIR = "/content/gdrive/MyDrive/prof_photos/osu_engineering_faculty"  # <-- adjust if you want
IMG_DIR = os.path.join(OUT_DIR, "photos")
CSV_PATH = os.path.join(OUT_DIR, "osu_engineering_faculty.csv")

SLEEP_PAGE = 0.6
SLEEP_IMG  = 0.3

HEADERS = {
    "User-Agent": "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122 Safari/537.36",
    "Accept-Language": "en-US,en;q=0.9",
}

EMAIL_RE = re.compile(r"[\w.+-]+@osu\.edu", re.IGNORECASE)


def is_data_uri(u: str) -> bool:
    return u.strip().startswith("data:")

def first_url_from_srcset(srcset: str) -> str:
    # srcset example: "url1 320w, url2 640w"
    for part in srcset.split(","):
        cand = part.strip().split(" ")[0].strip()
        if cand and not is_data_uri(cand):
            return cand
    return ""

def is_headshot_like(u: str) -> bool:
    u = u or ""
    return (
        "/sites/default/files/" in u
        or "people.engineering.osu.edu/sites/default/files/" in u
        or "profile-placeholder.png" in u
    )

def mount_drive():
    from google.colab import drive  # type: ignore
    drive.mount("/content/gdrive")

def make_session() -> requests.Session:
    s = requests.Session()
    retry = Retry(
        total=6,
        backoff_factor=1.0,
        status_forcelist=[429, 500, 502, 503, 504],
        allowed_methods=["GET"],
        raise_on_status=False,
    )
    adapter = HTTPAdapter(max_retries=retry, pool_connections=20, pool_maxsize=20)
    s.mount("https://", adapter)
    s.mount("http://", adapter)
    return s

def get_soup(session: requests.Session, url: str) -> BeautifulSoup:
    r = session.get(url, headers=HEADERS, timeout=30)
    r.raise_for_status()
    return BeautifulSoup(r.text, "html.parser")

def extract_last_page(soup: BeautifulSoup) -> int:
    # Find a pagination link containing "Last"
    for a in soup.find_all("a", href=True):
        txt = (a.get_text(" ", strip=True) or "").lower()
        if "last" in txt:
            full = urljoin(BASE, a["href"])
            q = urlparse(full).query
            m = re.search(r"(?:^|&)page=(\d+)(?:&|$)", q)
            if m:
                return int(m.group(1))
    # Fallback: at most 80 pages, stop when empty
    return 80

def safe_filename(s: str) -> str:
    s = (s or "").strip()
    s = re.sub(r"\s+", "_", s)                 # spaces -> underscore
    s = re.sub(r"[^a-zA-Z0-9._-]+", "_", s)    # keep safe chars
    s = s.strip("_")
    return s or "unknown"

def pick_best_image_url(container) -> str:
    # Prefer <a href> that looks like an image link
    for a in container.find_all("a", href=True):
        href = a["href"].strip()
        if not href or is_data_uri(href):
            continue
        full = urljoin("https://engineering.osu.edu", href)
        if is_headshot_like(full) or re.search(r"\.(jpe?g|png|webp)(\?.*)?$", full, re.IGNORECASE):
            return full

    # Then try <img>
    img = container.find("img")
    if not img:
        return ""

    candidates = []
    for k in ["data-src", "data-lazy-src", "data-original", "src"]:
        v = (img.get(k) or "").strip()
        if v and not is_data_uri(v):
            candidates.append(v)

    for k in ["data-srcset", "srcset"]:
        v = (img.get(k) or "").strip()
        u = first_url_from_srcset(v) if v else ""
        if u and not is_data_uri(u):
            candidates.append(u)

    for cand in candidates:
        full = urljoin("https://engineering.osu.edu", cand)
        if is_headshot_like(full) or re.search(r"\.(jpe?g|png|webp)(\?.*)?$", full, re.IGNORECASE):
            return full

    return ""

def find_row_container(start_tag) -> BeautifulSoup:
    """
    Climb up from a name tag to find a container that includes an osu.edu email.
    """
    cur = start_tag
    best = None
    for _ in range(15):
        if not cur or not hasattr(cur, "get_text"):
            break
        txt = cur.get_text(" ", strip=True)
        if EMAIL_RE.search(txt):
            best = cur
            if pick_best_image_url(cur):
                return cur
        cur = cur.parent
    return best if best is not None else start_tag

def profile_url_from_row(row: BeautifulSoup) -> str:
    m = EMAIL_RE.search(row.get_text(" ", strip=True))
    if not m:
        return ""
    osu_id = m.group(0).split("@")[0].strip()
    return f"{BASE}/people/{osu_id}"

def get_profile_image_url(session: requests.Session, profile_url: str) -> str:
    # Try to get a better image URL from the profile page
    try:
        soup = get_soup(session, profile_url)
        main = soup.find("main") or soup
        url = pick_best_image_url(main)
        return url
    except Exception:
        return ""

def download_image(session: requests.Session, image_url: str, dest_path: str, referer: str) -> bool:
    tmp = dest_path + ".part"
    os.makedirs(os.path.dirname(dest_path), exist_ok=True)

    headers = dict(HEADERS)
    headers["Referer"] = referer

    try:
        r = session.get(image_url, headers=headers, stream=True, timeout=60, allow_redirects=True)
        if r.status_code != 200:
            return False

        ctype = (r.headers.get("Content-Type") or "").lower()
        if "image" not in ctype:
            return False

        with open(tmp, "wb") as f:
            for chunk in r.iter_content(chunk_size=1024 * 64):
                if chunk:
                    f.write(chunk)

        if not os.path.exists(tmp) or os.path.getsize(tmp) < 200:
            try:
                os.remove(tmp)
            except Exception:
                pass
            return False

        os.replace(tmp, dest_path)
        return True

    except Exception:
        if os.path.exists(tmp) and os.path.getsize(tmp) >= 5_000:
            os.replace(tmp, dest_path)
            return True
        try:
            if os.path.exists(tmp):
                os.remove(tmp)
        except Exception:
            pass
        return False

def parse_directory_page(session: requests.Session, url: str):
    soup = get_soup(session, url)
    main = soup.find("main") or soup

    people = []
    for h2 in main.find_all("h2"):
        name = h2.get_text(" ", strip=True)
        if not name:
            continue

        row = find_row_container(h2)
        txt = row.get_text(" ", strip=True)
        m = EMAIL_RE.search(txt)
        if not m:
            continue

        profile_url = profile_url_from_row(row)
        image_url = pick_best_image_url(row)

        if (not image_url) or (image_url.rstrip("/") == PLACEHOLDER.rstrip("/")):
            if profile_url:
                image_url2 = get_profile_image_url(session, profile_url)
                if image_url2:
                    image_url = image_url2

        people.append((name, profile_url, image_url))
    return people, soup

def run():
    mount_drive()
    os.makedirs(IMG_DIR, exist_ok=True)
    os.makedirs(OUT_DIR, exist_ok=True)

    session = make_session()

    first_people, soup0 = parse_directory_page(session, LIST_URL)
    last_page = extract_last_page(soup0)

    seen = set()
    rows = []

    def handle_person(name, profile_url, image_url):
        key = profile_url or name
        if key in seen:
            return
        seen.add(key)
        if image_url and image_url.startswith("data:"):
            image_url = ""

        local_path = ""
        if image_url:
            # ====== ONLY CHANGE HERE: filename uses professor NAME only ======
            ext = os.path.splitext(urlparse(image_url).path)[1].lower()
            if ext not in [".jpg", ".jpeg", ".png", ".webp"]:
                ext = ".jpg"

            fname = safe_filename(name) + ext
            dest = os.path.join(IMG_DIR, fname)

            ok = download_image(session, image_url, dest, referer=profile_url or LIST_URL)
            if ok:
                local_path = os.path.relpath(dest, OUT_DIR)

            time.sleep(SLEEP_IMG)

        rows.append({
            "name": name,
            "profile_url": profile_url,
            "image_url": image_url,
            "local_path": local_path,
        })

    for (name, profile_url, image_url) in first_people:
        handle_person(name, profile_url, image_url)

    for page in tqdm(range(1, last_page + 1), desc="Pages"):
        url = LIST_URL + f"&page={page}"
        people, _ = parse_directory_page(session, url)
        if not people:
            break
        for (name, profile_url, image_url) in people:
            handle_person(name, profile_url, image_url)
        time.sleep(SLEEP_PAGE)

    df = pd.DataFrame(rows, columns=["name", "profile_url", "image_url", "local_path"])
    df.to_csv(CSV_PATH, index=False, encoding="utf-8-sig")

    print("Done:", len(df), "rows")
    print("Images folder:", IMG_DIR)
    print("CSV:", CSV_PATH)
    print("Rows with empty local_path:", int((df["local_path"] == "").sum()))

run()

"""WFU"""



import os, re, csv, time
from urllib.parse import urljoin, urlparse

import requests
from bs4 import BeautifulSoup

START_URL = "https://engineering.wfu.edu/people/faculty/"

OUT_ROOT = "/content/gdrive/MyDrive/prof_photos/wfu_engineering_faculty"
IMG_DIR  = os.path.join(OUT_ROOT, "photos")
CSV_PATH = os.path.join(OUT_ROOT, "wfu_engineering_faculty.csv")

os.makedirs(IMG_DIR, exist_ok=True)
os.makedirs(os.path.dirname(CSV_PATH), exist_ok=True)

HEADERS = {
    "User-Agent": "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 "
                  "(KHTML, like Gecko) Chrome/124.0.0.0 Safari/537.36",
    "Accept-Language": "en-US,en;q=0.9",
}

BAD_URL_KEYWORDS = ["logo", "icon", "sprite", "banner", "placeholder", "default", "loading", "spinner"]
BAD_ALT_KEYWORDS = ["logo", "icon", "banner", "placeholder", "default"]
SLEEP_IMG = 0.25  # be polite


def clean_person_name(raw: str) -> str:
    """
    Remove degree/cert suffixes like 'PhD', 'PE', etc. from displayed names.
    Examples:
      'Michael Gross, PhD' -> 'Michael Gross'
      'Patricia Clayton, PhD, PE' -> 'Patricia Clayton'
      'Saami K. Yazdani, PhD' -> 'Saami K. Yazdani'
    """
    s = (raw or "").strip()
    s = s.replace("\u00a0", " ")
    s = re.sub(r"\s+", " ", s).strip()

    # Drop trailing parenthetical degrees, e.g., "Name (PhD)"
    s = re.sub(r"\s*\(([^)]*)\)\s*$", "", s).strip()

    # Keep the first comma-separated chunk as the name
    parts = [p.strip() for p in s.split(",") if p.strip()]
    if not parts:
        return ""
    base = parts[0]

    deg_tokens = [
        "phd", "ph.d.", "pe", "p.e.", "md", "m.d.", "dds", "d.d.s.", "dvm", "d.v.m.",
        "jd", "j.d.", "mba", "m.b.a.", "ms", "m.s.", "ma", "m.a.", "mse", "m.s.e.",
        "meng", "m.eng", "me", "m.e.", "bs", "b.s.", "ba", "b.a.", "beng", "b.eng",
        "esq", "esquire", "pmp", "cfa", "frs", "rn", "r.n."
    ]

    # Remove trailing degree tokens even if they appear without commas: "Name PhD"
    for _ in range(3):
        base2 = re.sub(
            r"(?:,?\s+)(%s)\s*$" % "|".join(re.escape(t) for t in deg_tokens),
            "",
            base,
            flags=re.IGNORECASE
        ).strip()
        if base2 == base:
            break
        base = base2

    base = re.sub(r"[,â€“-]+\s*$", "", base).strip()
    return base


def safe_filename(name: str, max_len: int = 160) -> str:
    name = (name or "").strip()
    name = re.sub(r"\s+", " ", name)
    name = re.sub(r'[^A-Za-z0-9 .,_-]+', "", name).strip()
    name = name.replace(" ", "_")
    return (name[:max_len] if name else "unknown")


def ext_from_url(u: str) -> str:
    ext = os.path.splitext(urlparse(u).path)[1].lower()
    return ext if ext in [".jpg", ".jpeg", ".png", ".webp"] else ".jpg"


def unique_path(folder: str, base: str, ext: str) -> str:
    p = os.path.join(folder, base + ext)
    if not os.path.exists(p):
        return p
    k = 2
    while True:
        p2 = os.path.join(folder, f"{base}_{k}{ext}")
        if not os.path.exists(p2):
            return p2
        k += 1


def is_bad_image(url: str, alt: str = "") -> bool:
    if not url:
        return True
    low = url.lower()
    a = (alt or "").lower()
    if low.startswith("data:image"):
        return True
    if any(k in low for k in BAD_URL_KEYWORDS):
        return True
    if any(k in a for k in BAD_ALT_KEYWORDS):
        return True
    return False


def best_img_url_from_tag(img_tag, base_url: str) -> str:
    """
    Prefer srcset (largest) > data-src > src
    """
    if img_tag is None:
        return ""

    srcset = (img_tag.get("srcset") or "").strip()
    if srcset:
        parts = [p.strip().split(" ")[0] for p in srcset.split(",") if p.strip()]
        if parts:
            return urljoin(base_url, parts[-1])

    for k in ["data-src", "data-lazy-src", "data-original", "src"]:
        v = (img_tag.get(k) or "").strip()
        if v:
            return urljoin(base_url, v)

    return ""


def download_image(session: requests.Session, image_url: str, dest_path: str, referer: str) -> bool:
    headers = dict(HEADERS)
    headers["Referer"] = referer

    r = session.get(image_url, headers=headers, stream=True, timeout=30, allow_redirects=True)
    if r.status_code != 200:
        return False
    ctype = (r.headers.get("Content-Type") or "").lower()
    if "image" not in ctype:
        return False

    tmp = dest_path + ".part"
    with open(tmp, "wb") as f:
        for chunk in r.iter_content(chunk_size=1024 * 64):
            if chunk:
                f.write(chunk)

    if os.path.getsize(tmp) < 300:
        try:
            os.remove(tmp)
        except Exception:
            pass
        return False

    os.replace(tmp, dest_path)
    return True


def scrape_wfu_faculty():
    session = requests.Session()
    session.headers.update(HEADERS)

    resp = session.get(START_URL, timeout=30)
    resp.raise_for_status()
    soup = BeautifulSoup(resp.text, "html.parser")

    main = soup.find("main") or soup

    # 1) Primary strategy: find name-like links to /people/ profile pages
    candidates = []
    for a in main.select("a[href]"):
        text = (a.get_text(" ", strip=True) or "").strip()
        href = (a.get("href") or "").strip()
        if not href:
            continue

        full = urljoin(START_URL, href)
        low = full.lower()

        if "/people/" not in low:
            continue
        if low.rstrip("/").endswith("/people/faculty"):
            continue

        if not re.search(r"[A-Za-z]", text):
            continue
        if len(text.split()) < 2 or len(text) > 100:
            continue

        candidates.append((text, full, a))

    # Deduplicate by profile_url
    seen = set()
    records = []
    for name_raw, profile_url, a_tag in candidates:
        if profile_url in seen:
            continue
        seen.add(profile_url)

        container = a_tag.find_parent(["article", "li", "section", "div"]) or a_tag.parent
        img_tag = None
        if container:
            imgs = container.find_all("img")
            if imgs:
                img_tag = imgs[0]

        image_url = best_img_url_from_tag(img_tag, START_URL) if img_tag else ""
        image_alt = (img_tag.get("alt") if img_tag else "") or ""

        records.append({
            "name_raw": name_raw,
            "profile_url": profile_url,
            "image_url": image_url,
            "image_alt": image_alt
        })

    # 2) Fallback: if too few, try using image alt text as names
    if len(records) < 5:
        alt_records = []
        for img in main.select("img"):
            alt = (img.get("alt") or "").strip()
            if len(alt.split()) < 2 or len(alt) > 100:
                continue
            img_url = best_img_url_from_tag(img, START_URL)
            if not img_url:
                continue
            alt_records.append({
                "name_raw": alt,
                "profile_url": "",
                "image_url": img_url,
                "image_alt": alt
            })

        # Dedup by image_url
        m = {}
        for r in alt_records:
            if r["image_url"] and r["image_url"] not in m:
                m[r["image_url"]] = r
        records = list(m.values())

    rows = []
    saved = 0

    for r in records:
        name_raw = (r["name_raw"] or "").strip()
        name_clean = clean_person_name(name_raw)
        profile_url = r["profile_url"]
        image_url = (r["image_url"] or "").strip()
        image_alt = (r.get("image_alt") or "").strip()

        local_path = ""
        if image_url and (not is_bad_image(image_url, image_alt)):
            ext = ext_from_url(image_url)
            base = safe_filename(name_clean)  # cleaned name only
            dest = unique_path(IMG_DIR, base, ext)

            ok = False
            try:
                ok = download_image(session, image_url, dest, referer=profile_url or START_URL)
            except Exception:
                ok = False

            if ok:
                local_path = dest
                saved += 1
            time.sleep(SLEEP_IMG)

        rows.append({
            "name_raw": name_raw,
            "name": name_clean,
            "profile_url": profile_url,
            "image_url": image_url,
            "local_path": local_path
        })

    with open(CSV_PATH, "w", newline="", encoding="utf-8-sig") as f:
        w = csv.DictWriter(
            f,
            fieldnames=["name_raw", "name", "profile_url", "image_url", "local_path"]
        )
        w.writeheader()
        w.writerows(rows)

    print("Records extracted:", len(rows))
    print("Images saved:", saved)
    print("CSV:", CSV_PATH)
    print("Images folder:", IMG_DIR)
    print("Rows without local_path:", sum(1 for x in rows if not x["local_path"]))
    return rows


rows = scrape_wfu_faculty()
rows[:5]