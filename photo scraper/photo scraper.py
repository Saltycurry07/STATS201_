# -*- coding: utf-8 -*-
"""stats201 project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1YNGPe0j-8sLiJF1rkD1tohj4GrK1mQI4
"""

import os
import re
import time
from urllib.parse import urljoin, urlparse

import requests
from bs4 import BeautifulSoup

START_URL = "https://cs.nyu.edu/dynamic/people/faculty/type/20/"  # All Faculty
OUT_DIR = "nyu_cs_faculty_photos"
SLEEP_SEC = 0.6

os.makedirs(OUT_DIR, exist_ok=True)

session = requests.Session()
session.headers.update({

    "User-Agent": "Mozilla/5.0 (compatible; FacultyPhotoDownloader/1.0; +you@example.com)"
})

resp = session.get(START_URL, timeout=30)
resp.raise_for_status()

soup = BeautifulSoup(resp.text, "html.parser")

img_urls = set()

for tag in soup.select("a[href], img[src], img[data-src], img[data-lazy-src]"):
    for attr in ("href", "src", "data-src", "data-lazy-src"):
        u = tag.get(attr)
        if not u:
            continue
        u = urljoin(START_URL, u)
        if ("/media/faculty_photos/" in u and re.search(r"\.(jpg|jpeg|png|webp)$", u, re.I)):
            img_urls.add(u)

print(f"Found {len(img_urls)} photo links")

for u in sorted(img_urls):
    filename = os.path.basename(urlparse(u).path)
    save_path = os.path.join(OUT_DIR, filename)

    if os.path.exists(save_path):
        continue

    r = session.get(u, stream=True, timeout=30)
    r.raise_for_status()

    ctype = (r.headers.get("Content-Type") or "").lower()
    if "image" not in ctype:
        print("Skip (not image):", u, ctype)
        continue

    with open(save_path, "wb") as f:
        for chunk in r.iter_content(chunk_size=1024 * 64):
            if chunk:
                f.write(chunk)

    print("Saved:", filename)
    time.sleep(SLEEP_SEC)

from google.colab import drive
drive.mount('/content/drive')

!cp -r /content/nyu_cs_faculty_photos /content/drive/MyDrive/

import os, re, asyncio, hashlib
from urllib.parse import urlparse
from playwright.async_api import async_playwright

START_URL = "https://samueli.ucla.edu/search-faculty/#cs"
OUT_DIR = "/content/drive/MyDrive/prof_photos/ucla_samueli_cs"
MAX_IMAGES = None

os.makedirs(OUT_DIR, exist_ok=True)

def safe_filename(s: str) -> str:
    s = re.sub(r"[^a-zA-Z0-9._-]+", "_", (s or "").strip())
    return s[:180] if s else hashlib.md5(os.urandom(16)).hexdigest()

async def scrape_ucla_samueli_cs_photos():
    async with async_playwright() as p:
        browser = await p.chromium.launch(
            headless=True,
            args=[
                "--no-sandbox",
                "--disable-setuid-sandbox",
                "--disable-dev-shm-usage",
                "--disable-gpu",
                "--single-process",
                "--no-zygote",
            ],
        )
        context = await browser.new_context(
            user_agent=(
                "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 "
                "(KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36"
            )
        )
        page = await context.new_page()


        await page.goto(START_URL, wait_until="domcontentloaded", timeout=60000)

        await page.wait_for_timeout(2000)
        await page.wait_for_function("document.images.length > 5", timeout=60000)
        await page.wait_for_timeout(2500)

        print("candidate images found:", len(imgs))

        if MAX_IMAGES is not None:
            imgs = imgs[:MAX_IMAGES]

        saved = 0
        skipped = 0
        for i, it in enumerate(imgs, 1):
            url = it["src"]
            alt = it.get("alt") or ""
            hint = alt if alt.strip() else f"ucla_cs_{i}"
            filename = safe_filename(hint) + os.path.splitext(urlparse(url).path)[1].lower()
            if not filename.lower().endswith((".jpg", ".jpeg", ".png", ".webp")):
                filename += ".jpg"

            path = os.path.join(OUT_DIR, filename)
            if os.path.exists(path):
                skipped += 1
                continue

            try:
                resp = await context.request.get(url, timeout=60000)
                if not resp.ok:
                    skipped += 1
                    continue
                ctype = (resp.headers.get("content-type") or "").lower()
                if "image" not in ctype:
                    skipped += 1
                    continue
                data = await resp.body()
                with open(path, "wb") as f:
                    f.write(data)
                saved += 1
            except Exception:
                skipped += 1

        await context.close()
        await browser.close()

        print(f"done. saved={saved}, skipped={skipped}, out_dir={OUT_DIR}")
        return imgs

imgs = await scrape_ucla_samueli_cs_photos()
print("preview:", imgs[:5])
