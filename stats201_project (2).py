# -*- coding: utf-8 -*-
"""stats201 project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1YNGPe0j-8sLiJF1rkD1tohj4GrK1mQI4
"""

import os
import re
import time
from urllib.parse import urljoin, urlparse

import requests
from bs4 import BeautifulSoup

START_URL = "https://cs.nyu.edu/dynamic/people/faculty/type/20/"  # All Faculty
OUT_DIR = "nyu_cs_faculty_photos"
SLEEP_SEC = 0.6

os.makedirs(OUT_DIR, exist_ok=True)

session = requests.Session()
session.headers.update({

    "User-Agent": "Mozilla/5.0 (compatible; FacultyPhotoDownloader/1.0; +you@example.com)"
})

resp = session.get(START_URL, timeout=30)
resp.raise_for_status()

soup = BeautifulSoup(resp.text, "html.parser")

img_urls = set()

for tag in soup.select("a[href], img[src], img[data-src], img[data-lazy-src]"):
    for attr in ("href", "src", "data-src", "data-lazy-src"):
        u = tag.get(attr)
        if not u:
            continue
        u = urljoin(START_URL, u)
        if ("/media/faculty_photos/" in u and re.search(r"\.(jpg|jpeg|png|webp)$", u, re.I)):
            img_urls.add(u)

print(f"Found {len(img_urls)} photo links")

for u in sorted(img_urls):
    filename = os.path.basename(urlparse(u).path)
    save_path = os.path.join(OUT_DIR, filename)

    if os.path.exists(save_path):
        continue

    r = session.get(u, stream=True, timeout=30)
    r.raise_for_status()

    ctype = (r.headers.get("Content-Type") or "").lower()
    if "image" not in ctype:
        print("Skip (not image):", u, ctype)
        continue

    with open(save_path, "wb") as f:
        for chunk in r.iter_content(chunk_size=1024 * 64):
            if chunk:
                f.write(chunk)

    print("Saved:", filename)
    time.sleep(SLEEP_SEC)

from google.colab import drive
drive.mount('/content/drive')

!cp -r /content/nyu_cs_faculty_photos /content/drive/MyDrive/

# 1) Install (run once per runtime)
!pip -q install -U playwright
!playwright install --with-deps chromium

import os, re, asyncio, hashlib
from urllib.parse import urlparse
from playwright.async_api import async_playwright

START_URL = "https://samueli.ucla.edu/search-faculty/#cs"
OUT_DIR = "/content/drive/MyDrive/prof_photos/ucla_samueli_cs"
MAX_IMAGES = None

os.makedirs(OUT_DIR, exist_ok=True)

def safe_filename(s: str) -> str:
    s = re.sub(r"[^a-zA-Z0-9._-]+", "_", (s or "").strip())
    return s[:180] if s else hashlib.md5(os.urandom(16)).hexdigest()

async def scrape_ucla_samueli_cs_photos():
    async with async_playwright() as p:
        browser = await p.chromium.launch(
            headless=True,
            args=[
                "--no-sandbox",
                "--disable-setuid-sandbox",
                "--disable-dev-shm-usage",
                "--disable-gpu",
                "--single-process",
                "--no-zygote",
            ],
        )
        context = await browser.new_context(
            user_agent=(
                "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 "
                "(KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36"
            )
        )
        page = await context.new_page()


        await page.goto(START_URL, wait_until="domcontentloaded", timeout=60000)

        await page.wait_for_timeout(2000)
        await page.wait_for_function("document.images.length > 5", timeout=60000)
        await page.wait_for_timeout(2500)

        # The original code had `imgs` undefined at this point. Corrected by fetching all image elements
        imgs = await page.evaluate("""
            () => {
                const base = location.href;
                const abs = (u) => { try { return new URL(u, base).href; } catch(e) { return null; } };
                return Array.from(document.querySelectorAll('img'))
                    .map(img => ({ src: abs(img.currentSrc || img.src || img.getAttribute('data-src') || img.getAttribute('data-lazy-src')), alt: img.alt }))
                    .filter(img => img.src && (img.src.includes('profile') || img.src.includes('faculty') || img.src.includes('photo')));
            }
        """)

        print("candidate images found:", len(imgs))

        if MAX_IMAGES is not None:
            imgs = imgs[:MAX_IMAGES]

        saved = 0
        skipped = 0
        for i, it in enumerate(imgs, 1):
            url = it["src"]
            alt = it.get("alt") or ""
            hint = alt if alt.strip() else f"ucla_cs_{i}"
            filename = safe_filename(hint) + os.path.splitext(urlparse(url).path)[1].lower()
            if not filename.lower().endswith((".jpg", ".jpeg", ".png", ".webp", ".gif")):
                filename += ".jpg"

            path = os.path.join(OUT_DIR, filename)
            if os.path.exists(path):
                skipped += 1
                continue

            try:
                resp = await context.request.get(url, timeout=60000)
                if not resp.ok:
                    skipped += 1
                    continue
                ctype = (resp.headers.get("content-type") or "").lower()
                if "image" not in ctype:
                    skipped += 1
                    continue
                data = await resp.body()
                with open(path, "wb") as f:
                    f.write(data)
                saved += 1
            except Exception:
                skipped += 1

        await context.close()
        await browser.close()

        print(f"done. saved={saved}, skipped={skipped}, out_dir={OUT_DIR}")
        return imgs

imgs = await scrape_ucla_samueli_cs_photos()
print("preview:", imgs[:5])

# ============================================================
# NYU CS faculty directory — download photos named by professor
# ONLY download images. NO CSV.
# Filenames: Professor_Name.jpg, Professor_Name_2.jpg, ...
# URL: https://cs.nyu.edu/dynamic/people/faculty/
# ============================================================

# 1) Install (run once per runtime)
!pip -q install -U playwright
!playwright install --with-deps chromium

# 2) Mount Drive (optional but recommended)
from google.colab import drive
drive.mount("/content/drive")

import os, re
from urllib.parse import urlparse
from playwright.async_api import async_playwright

START_URL = "https://cs.nyu.edu/dynamic/people/faculty/"
OUT_DIR  = "/content/drive/MyDrive/prof_photos/nyu_cs_faculty_named"  # change if you want

SCROLL_TIMES = 20
WAIT_MS_AFTER_SCROLL = 700
MAX_PEOPLE = None  # set to e.g. 300 if you want a cap

os.makedirs(OUT_DIR, exist_ok=True)

def safe_filename(name: str, max_len: int = 120) -> str:
    name = (name or "").strip()
    name = re.sub(r"\s+", " ", name)                 # collapse spaces
    name = re.sub(r"[^a-zA-Z0-9._ -]+", "", name)    # remove weird chars
    name = name.replace(" ", "_")
    return name[:max_len] if name else "unknown"

def ext_from_url(u: str) -> str:
    ext = os.path.splitext(urlparse(u).path)[1].lower()
    return ext if ext in [".jpg", ".jpeg", ".png", ".webp"] else ".jpg"

def unique_path(out_dir: str, base: str, ext: str) -> tuple[str, str]:
    """Return (filename, path) that doesn't overwrite existing files."""
    filename = f"{base}{ext}"
    path = os.path.join(out_dir, filename)
    k = 2
    while os.path.exists(path):
        filename = f"{base}_{k}{ext}"
        path = os.path.join(out_dir, filename)
        k += 1
    return filename, path

async def download_named_images():
    async with async_playwright() as p:
        browser = await p.chromium.launch(
            headless=True,
            args=[
                "--no-sandbox",
                "--disable-setuid-sandbox",
                "--disable-dev-shm-usage",
                "--disable-gpu",
                "--single-process",
                "--no-zygote",
            ],
        )
        context = await browser.new_context(
            user_agent=(
                "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 "
                "(KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36"
            )
        )
        page = await context.new_page()

        print("[info] goto:", START_URL)
        await page.goto(START_URL, wait_until="domcontentloaded", timeout=60000)
        await page.wait_for_timeout(2000)

        # scroll to load people
        for _ in range(SCROLL_TIMES):
            await page.mouse.wheel(0, 2600)
            await page.wait_for_timeout(WAIT_MS_AFTER_SCROLL)

        # Extract pairs (name, image_url) from rendered DOM
        records = await page.evaluate("""
        () => {
          const base = location.href;
          const abs = (u) => { try { return new URL(u, base).href; } catch(e) { return null; } };

          function pickName(container) {
            if (!container) return null;
            const links = Array.from(container.querySelectorAll("a[href]"))
              .map(a => (a.textContent || "").trim())
              .filter(t => t && t.toLowerCase() !== "image");
            const nameLike = links.find(t => t.split(/\\s+/).length >= 2) || links[0];
            return nameLike || null;
          }

          const out = [];

          // A) photo links directly to /media/faculty_photos/
          const photoAnchors = Array.from(document.querySelectorAll("a[href]"))
            .filter(a => (a.getAttribute("href") || "").includes("/media/faculty_photos/"));

          for (const a of photoAnchors) {
            const image_url = abs(a.getAttribute("href"));
            if (!image_url) continue;
            const container = a.closest("li, article, section, div") || a.parentElement || document.body;
            const name = pickName(container);
            out.push({ name, image_url });
          }

          // B) img tags pointing to /media/faculty_photos/
          const imgs = Array.from(document.querySelectorAll("img"));
          for (const img of imgs) {
            const current = img.currentSrc || img.src || "";
            let best = current;

            if (!best) {
              const srcset = img.getAttribute("srcset") || "";
              if (srcset) {
                const parts = srcset.split(",").map(s => s.trim().split(" ")[0]).filter(Boolean);
                if (parts.length) best = parts[parts.length - 1];
              }
            }
            if (!best || !best.includes("/media/faculty_photos/")) continue;

            const image_url = abs(best);
            if (!image_url) continue;

            const container = img.closest("li, article, section, div") || img.parentElement || document.body;
            const name = pickName(container);
            out.push({ name, image_url });
          }

          // Deduplicate by image_url
          const seen = new Set();
          const uniq = [];
          for (const r of out) {
            if (!r.image_url || seen.has(r.image_url)) continue;
            seen.add(r.image_url);
            uniq.push(r);
          }
          return uniq;
        }
        """)

        print("[info] found image records:", len(records))
        if MAX_PEOPLE is not None:
            records = records[:MAX_PEOPLE]

        saved = 0
        skipped = 0
        unknown = 0

        for i, r in enumerate(records, 1):
            name = (r.get("name") or "").strip()
            image_url = r.get("image_url")

            if not name:
                name = f"unknown_{i}"
                unknown += 1

            base = safe_filename(name)
            ext  = ext_from_url(image_url)
            filename, path = unique_path(OUT_DIR, base, ext)

            try:
                resp = await context.request.get(image_url, timeout=60000)
                if not resp.ok:
                    skipped += 1
                    continue

                ctype = (resp.headers.get("content-type") or "").lower()
                if "image" not in ctype:
                    skipped += 1
                    continue

                data = await resp.body()
                with open(path, "wb") as f:
                    f.write(data)

                saved += 1

            except Exception:
                skipped += 1

        await context.close()
        await browser.close()

        print(f"[done] saved={saved}, skipped={skipped}, unknown_names={unknown}")
        print("[done] images dir:", OUT_DIR)

await download_named_images()

"""NYU CS (with csv directory)"""

# ============================================================
# NYU CS faculty directory — download photos + export CSV
# URL: https://cs.nyu.edu/dynamic/people/faculty/
# Output: images folder + CSV(name, profile_url, image_url, local_path)
# Works in Google Colab (dynamic site) via Playwright Async API.
# ============================================================

# 1) Install (run once per runtime)
!pip -q install -U playwright
!playwright install --with-deps chromium

# 2) Mount Drive (optional but recommended)
from google.colab import drive
drive.mount("/content/drive")

import os, re, csv, hashlib, asyncio
from urllib.parse import urlparse
from playwright.async_api import async_playwright

START_URL = "https://cs.nyu.edu/dynamic/people/faculty/"
OUT_DIR  = "/content/drive/MyDrive/prof_photos/nyu_cs_faculty"      # change if you want
CSV_PATH = "/content/drive/MyDrive/prof_photos/nyu_cs_faculty.csv"  # change if you want

SCROLL_TIMES = 18          # increase if not all people load
WAIT_MS_AFTER_SCROLL = 600 # adjust if page loads slowly
MAX_PEOPLE = None          # e.g. 300; keep None for all found

os.makedirs(OUT_DIR, exist_ok=True)
os.makedirs(os.path.dirname(CSV_PATH), exist_ok=True)

def safe_filename(s: str) -> str:
    s = re.sub(r"[^a-zA-Z0-9._-]+", "_", (s or "").strip())
    return s[:180] if s else hashlib.md5(os.urandom(16)).hexdigest()

def ext_from_url(u: str) -> str:
    ext = os.path.splitext(urlparse(u).path)[1].lower()
    return ext if ext in [".jpg", ".jpeg", ".png", ".webp"] else ".jpg"

async def scrape_nyu_faculty_with_photos():
    async with async_playwright() as p:
        browser = await p.chromium.launch(
            headless=True,
            args=[
                "--no-sandbox",
                "--disable-setuid-sandbox",
                "--disable-dev-shm-usage",
                "--disable-gpu",
                "--single-process",
                "--no-zygote",
            ],
        )
        context = await browser.new_context(
            user_agent=(
                "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 "
                "(KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36"
            )
        )
        page = await context.new_page()

        print("[info] goto:", START_URL)
        await page.goto(START_URL, wait_until="domcontentloaded", timeout=60000)
        await page.wait_for_timeout(2000)

        # Scroll to trigger lazy load / pagination
        for _ in range(SCROLL_TIMES):
            await page.mouse.wheel(0, 2400)
            await page.wait_for_timeout(WAIT_MS_AFTER_SCROLL)

        # Extract records from rendered DOM
        records = await page.evaluate("""
        () => {
          const base = location.href;
          const abs = (u) => {
            try { return new URL(u, base).href; } catch(e) { return null; }
          };

          // Helper: pick a "name-like" text from anchors within a container
          function pickNameAndProfile(container) {
            const links = Array.from(container.querySelectorAll("a[href]"))
              .map(a => ({ text: (a.textContent||"").trim(), href: abs(a.getAttribute("href")) }))
              .filter(x => x.href && x.text && x.text.toLowerCase() !== "image");

            // Prefer 2+ words (looks like a person name)
            const nameLike = links.find(x => x.text.split(/\\s+/).length >= 2) || links[0];
            return nameLike ? { name: nameLike.text, profile_url: nameLike.href } : { name: null, profile_url: null };
          }

          // Strategy A: "Image" anchors or direct faculty photo links
          const photoAnchors = Array.from(document.querySelectorAll("a[href]"))
            .filter(a => (a.getAttribute("href") || "").includes("/media/faculty_photos/"));

          // Strategy B: <img> tags that point to faculty_photos (or srcset)
          const photoImgs = Array.from(document.querySelectorAll("img"))
            .map(img => {
              const src = img.currentSrc || img.src || "";
              const srcset = img.getAttribute("srcset") || "";
              let best = src;
              if (!best && srcset) {
                const parts = srcset.split(",").map(s => s.trim().split(" ")[0]).filter(Boolean);
                if (parts.length) best = parts[parts.length - 1];
              }
              return { el: img, url: best };
            })
            .filter(x => (x.url || "").includes("/media/faculty_photos/"));

          const out = [];

          // From anchors
          for (const a of photoAnchors) {
            const image_url = abs(a.getAttribute("href"));
            if (!image_url) continue;

            const container = a.closest("li, article, div") || a.parentElement || document.body;
            const { name, profile_url } = pickNameAndProfile(container);

            out.push({ name, profile_url, image_url });
          }

          // From imgs
          for (const x of photoImgs) {
            const image_url = abs(x.url);
            if (!image_url) continue;

            const container = x.el.closest("li, article, div") || x.el.parentElement || document.body;
            const { name, profile_url } = pickNameAndProfile(container);

            out.push({ name, profile_url, image_url });
          }

          // De-duplicate by image_url; keep first non-null name/profile if possible
          const byImg = new Map();
          for (const r of out) {
            if (!r.image_url) continue;
            if (!byImg.has(r.image_url)) byImg.set(r.image_url, r);
            else {
              const cur = byImg.get(r.image_url);
              if ((!cur.name && r.name) || (!cur.profile_url && r.profile_url)) {
                byImg.set(r.image_url, { ...cur, ...r });
              }
            }
          }

          return Array.from(byImg.values());
        }
        """)

        print("[info] records found:", len(records))
        if len(records) == 0:
            await context.close()
            await browser.close()
            raise RuntimeError("No records found. The DOM structure may have changed; need a more specific selector.")

        if MAX_PEOPLE is not None:
            records = records[:MAX_PEOPLE]

        # Download images + write rows
        rows = []
        saved = 0
        for i, r in enumerate(records, 1):
            name = (r.get("name") or f"unknown_{i}").strip()
            profile_url = r.get("profile_url")
            image_url = r.get("image_url")

            filename = safe_filename(name) + ext_from_url(image_url)
            local_path = os.path.join(OUT_DIR, filename)

            if not os.path.exists(local_path):
                try:
                    resp = await context.request.get(image_url, timeout=60000)
                    if resp.ok:
                        ctype = (resp.headers.get("content-type") or "").lower()
                        if "image" in ctype:
                            data = await resp.body()
                            with open(local_path, "wb") as f:
                                f.write(data)
                            saved += 1
                        else:
                            local_path = None
                    else:
                        local_path = None
                except Exception:
                    local_path = None

            rows.append({
                "name": name,
                "profile_url": profile_url,
                "image_url": image_url,
                "local_path": local_path
            })

        # Write CSV
        with open(CSV_PATH, "w", newline="", encoding="utf-8") as f:
            w = csv.DictWriter(f, fieldnames=["name", "profile_url", "image_url", "local_path"])
            w.writeheader()
            w.writerows(rows)

        await context.close()
        await browser.close()

        print(f"[done] saved images: {saved}/{len(rows)}")
        print("CSV:", CSV_PATH)
        print("Images:", OUT_DIR)

        return rows

rows = await scrape_nyu_faculty_with_photos()
print("preview (first 5):")
for r in rows[:5]:
    print(r["name"], "|", r["image_url"])

"""NYU Art (with csv directory)"""

import os, re, csv, hashlib
from urllib.parse import urlparse
from playwright.async_api import async_playwright

START_URL = "https://tisch.nyu.edu/drama/faculty/ft-faculty"
OUT_DIR  = "/content/drive/MyDrive/prof_photos/nyu_tisch_drama_ft"     # 图片输出目录
CSV_PATH = "/content/drive/MyDrive/prof_photos/nyu_tisch_drama_ft.csv" # CSV 输出路径

os.makedirs(OUT_DIR, exist_ok=True)
os.makedirs(os.path.dirname(CSV_PATH), exist_ok=True)

def safe_filename(s: str) -> str:
    s = re.sub(r"[^a-zA-Z0-9._-]+", "_", (s or "").strip())
    return s[:180] if s else hashlib.md5(os.urandom(16)).hexdigest()

def guess_ext_from_url(u: str) -> str:
    ext = os.path.splitext(urlparse(u).path)[1].lower()
    return ext if ext in [".jpg",".jpeg",".png",".webp"] else ".jpg"

async def scrape_and_download():
    async with async_playwright() as p:
        browser = await p.chromium.launch(
            headless=True,
            args=[
                "--no-sandbox",
                "--disable-setuid-sandbox",
                "--disable-dev-shm-usage",
                "--disable-gpu",
                "--single-process",
                "--no-zygote",
            ],
        )
        context = await browser.new_context(
            user_agent=(
                "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 "
                "(KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36"
            )
        )
        page = await context.new_page()

        # 1) 打开页面并等待渲染
        await page.goto(START_URL, wait_until="domcontentloaded", timeout=60000)
        await page.wait_for_timeout(2500)

        # 2) 等到页面出现一定数量的图片（避免拿到空 DOM）
        try:
            await page.wait_for_function("document.images.length > 5", timeout=20000)
        except Exception:
            pass

        # 3) 在浏览器里跑 JS：抽取 (name, profile_url, image_url)
        #    通用策略：对每个 <img> 找最近的容器（article/li/section/div），
        #    再从该容器里找最像人名的链接/标题文本。
        records = await page.evaluate("""
        () => {
          const base = location.href;
          const abs = (u) => { try { return new URL(u, base).href; } catch(e) { return null; } };

          // 从 img 获取最可能的图片 URL（currentSrc/src/srcset）
          function getImgUrl(img) {
            let u = img.currentSrc || img.src || "";
            if (!u) {
              const srcset = img.getAttribute("srcset") || "";
              if (srcset) {
                const parts = srcset.split(",").map(s => s.trim().split(" ")[0]).filter(Boolean);
                if (parts.length) u = parts[parts.length - 1];
              }
            }
            return u ? abs(u) : null;
          }

          // 判断文本是否像“人名”
          function looksLikeName(t) {
            if (!t) return false;
            t = t.trim();
            // 至少两个词，且包含字母
            if (!/[A-Za-z]/.test(t)) return false;
            const words = t.split(/\\s+/);
            if (words.length < 2) return false;
            // 过滤掉明显非人名的导航/按钮词
            const bad = ["learn", "more", "view", "faculty", "staff", "department", "program", "contact", "email"];
            const low = t.toLowerCase();
            if (bad.some(b => low === b || low.startsWith(b + " "))) return false;
            return true;
          }

          // 在容器里挑一个最像姓名的 (name, profile_url)
          function pickNameAndProfile(container) {
            if (!container) return { name: null, profile_url: null };

            // 优先：标题标签
            const headings = Array.from(container.querySelectorAll("h1,h2,h3,h4,h5"))
              .map(h => (h.textContent || "").trim())
              .filter(looksLikeName);
            if (headings.length) {
              // profile_url：如果标题里包了链接就取链接
              const h = container.querySelector("h1 a[href],h2 a[href],h3 a[href],h4 a[href],h5 a[href]");
              return { name: headings[0], profile_url: h ? abs(h.getAttribute("href")) : null };
            }

            // 其次：所有链接文本
            const links = Array.from(container.querySelectorAll("a[href]"))
              .map(a => ({
                text: (a.textContent || "").trim(),
                href: abs(a.getAttribute("href"))
              }))
              .filter(x => x.href && looksLikeName(x.text));

            if (links.length) return { name: links[0].text, profile_url: links[0].href };

            // 最后：用 img 的 alt 兜底
            const img = container.querySelector("img");
            const alt = img ? (img.alt || "").trim() : "";
            return looksLikeName(alt) ? { name: alt, profile_url: null } : { name: null, profile_url: null };
          }

          const out = [];

          // 遍历所有 img，过滤掉太小/装饰图
          const imgs = Array.from(document.querySelectorAll("img"));
          for (const img of imgs) {
            const w = img.naturalWidth || 0;
            const h = img.naturalHeight || 0;
            if (w && h && (w < 120 || h < 120)) continue;

            const image_url = getImgUrl(img);
            if (!image_url) continue;

            const low = image_url.toLowerCase();
            if (low.includes("logo") || low.includes("icon") || low.includes("sprite") || low.includes("banner")) continue;
            if (low.startsWith("data:image")) continue;

            // 找“卡片容器”
            const container = img.closest("article, li, section, div") || img.parentElement;
            const { name, profile_url } = pickNameAndProfile(container);

            out.push({ name, profile_url, image_url });
          }

          // 去重（优先按 image_url；同一 image_url 取更完整的 name/profile）
          const map = new Map();
          for (const r of out) {
            if (!r.image_url) continue;
            if (!map.has(r.image_url)) {
              map.set(r.image_url, r);
            } else {
              const cur = map.get(r.image_url);
              const better = {
                name: cur.name || r.name,
                profile_url: cur.profile_url || r.profile_url,
                image_url: cur.image_url
              };
              map.set(r.image_url, better);
            }
          }

          // 再过滤一次：必须有 name 或 alt-like name（否则太多装饰图）
          const res = Array.from(map.values()).filter(r => r.name && r.name.trim().length >= 3);
          return res;
        }
        """)

        print("records found:", len(records))
        if len(records) == 0:
            await context.close()
            await browser.close()
            raise RuntimeError("No records found. The page structure may have changed; need different selectors.")

        # 4) 下载图片 + 写入本地路径
        rows = []
        saved = 0

        for i, r in enumerate(records, 1):
            name = (r.get("name") or f"unknown_{i}").strip()
            profile_url = r.get("profile_url")
            image_url = r.get("image_url")

            ext = guess_ext_from_url(image_url)
            filename = safe_filename(name) + ext
            local_path = os.path.join(OUT_DIR, filename)

            if not os.path.exists(local_path):
                try:
                    resp = await context.request.get(image_url, timeout=60000)
                    if resp.ok:
                        ctype = (resp.headers.get("content-type") or "").lower()
                        if "image" in ctype:
                            data = await resp.body()
                            with open(local_path, "wb") as f:
                                f.write(data)
                            saved += 1
                        else:
                            local_path = None
                    else:
                        local_path = None
                except Exception:
                    local_path = None

            rows.append({
                "name": name,
                "profile_url": profile_url,
                "image_url": image_url,
                "local_path": local_path
            })

        # 5) 写 CSV
        with open(CSV_PATH, "w", newline="", encoding="utf-8") as f:
            w = csv.DictWriter(f, fieldnames=["name", "profile_url", "image_url", "local_path"])
            w.writeheader()
            w.writerows(rows)

        await context.close()
        await browser.close()

        print(f"saved images: {saved}/{len(rows)}")
        print("CSV:", CSV_PATH)
        print("Images:", OUT_DIR)

        return rows

rows = await scrape_and_download()
print("preview:", rows[:5])

"""Data append & merge"""

import pandas as pd

cs_path = "/content/nyu_cs_faculty_scored.csv"
art_path = "/content/nyu_art_faculty_scored.csv"

cs = pd.read_csv(cs_path)
art = pd.read_csv(art_path)

# 写全称 + 加 school
cs["dept"] = "Computer Science"
art["dept"] = "Art"
cs["school"] = "NYU"
art["school"] = "NYU"

combined = pd.concat([cs, art], axis=0, ignore_index=True, sort=False)

out_path = "/content/nyu_cs_art_faculty_scored.csv"
combined.to_csv(out_path, index=False, encoding="utf-8-sig")

print("Saved:", out_path)
print("Rows:", combined.shape[0], "Cols:", combined.shape[1])

import pandas as pd
import re

df1 = pd.read_csv("rmp_ucla_nyu_professors.csv")
df2 = pd.read_csv("nyu_cs_art_faculty_scored.csv")

def norm_name(x):
    if pd.isna(x):
        return ""
    x = str(x).strip().lower()
    x = re.sub(r"[^\w\s]", " ", x)
    x = re.sub(r"\s+", " ", x).strip()
    return x

df1["_key"] = df1["professor"].map(norm_name)
df2["_key"] = df2["name"].map(norm_name)

last3_cols = [c for c in df1.columns if c not in ["_key"]][-3:]
df1_small = df1[["professor"] + last3_cols + ["_key"]].copy()

merged = df2.merge(df1_small, on="_key", how="inner").drop(columns=["_key"])
merged.to_csv("table2_matched_with_table1_last3.csv", index=False, encoding="utf-8-sig")

"""UCLA CS (with csv directory)"""

# ============================
# UCLA Samueli faculty (CS) scraper
# URL: https://samueli.ucla.edu/search-faculty/#cs
# Output: images + CSV(name, profile_url, image_url, local_path)
# ============================

# ---- 0) Install Playwright (Colab run once) ----
!pip -q install -U playwright
!playwright install --with-deps chromium

# ---- 1) (Optional) Mount Google Drive ----
from google.colab import drive
drive.mount("/content/drive")

import os, re, csv, hashlib
from urllib.parse import urlparse
from playwright.async_api import async_playwright

START_URL = "https://samueli.ucla.edu/search-faculty/#cs"

# Change these paths as you like
OUT_DIR  = "/content/drive/MyDrive/prof_photos/ucla_samueli_cs"
CSV_PATH = "/content/drive/MyDrive/prof_photos/ucla_samueli_cs_faculty.csv"

DOWNLOAD_IMAGES = True          # 若只要 CSV，不下载图片就改 False
SCROLL_TIMES = 25               # 页面可能是懒加载，必要时加大
WAIT_MS_AFTER_SCROLL = 700
MAX_PEOPLE = None               # e.g., 300; None = all

os.makedirs(OUT_DIR, exist_ok=True)
os.makedirs(os.path.dirname(CSV_PATH), exist_ok=True)

def safe_filename(s: str) -> str:
    s = (s or "").strip()
    s = re.sub(r"\s+", " ", s)
    s = re.sub(r"[^a-zA-Z0-9._ -]+", "", s)
    s = s.replace(" ", "_")
    return s[:160] if s else hashlib.md5(os.urandom(16)).hexdigest()

def ext_from_url(u: str) -> str:
    ext = os.path.splitext(urlparse(u).path)[1].lower()
    return ext if ext in [".jpg", ".jpeg", ".png", ".webp"] else ".jpg"

async def scrape_ucla_cs():
    async with async_playwright() as p:
        browser = await p.chromium.launch(
            headless=True,
            args=[
                "--no-sandbox",
                "--disable-setuid-sandbox",
                "--disable-dev-shm-usage",
                "--disable-gpu",
                "--single-process",
                "--no-zygote",
            ],
        )
        context = await browser.new_context(
            user_agent=(
                "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 "
                "(KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36"
            )
        )
        page = await context.new_page()

        print("[info] goto:", START_URL)
        await page.goto(START_URL, wait_until="domcontentloaded", timeout=60000)

        # 让 JS 渲染起来
        await page.wait_for_timeout(3000)

        # 尝试等到页面上出现一些 faculty cards/people links/images
        # （不同版本结构不一样，用多条件兜底）
        try:
            await page.wait_for_function(
                """() => {
                    const hasPeopleLinks = document.querySelectorAll('a[href*="/people/"], a[href*="/faculty/"], a[href*="/directory/"]').length > 5;
                    const hasImgs = document.images.length > 10;
                    return hasPeopleLinks || hasImgs;
                }""",
                timeout=20000
            )
        except Exception:
            pass

        # 滚动触发懒加载（如果有）
        for _ in range(SCROLL_TIMES):
            await page.mouse.wheel(0, 2400)
            await page.wait_for_timeout(WAIT_MS_AFTER_SCROLL)

        # ---- Extract records from rendered DOM ----
        records = await page.evaluate("""
        () => {
          const base = location.href;
          const abs = (u) => { try { return new URL(u, base).href; } catch(e) { return null; } };

          const badImgKw = ["logo","icon","sprite","banner","loading","spinner"];
          const looksLikeName = (t) => {
            if (!t) return false;
            t = t.trim();
            if (!/[A-Za-z]/.test(t)) return false;
            const words = t.split(/\\s+/);
            if (words.length < 2) return false;
            const low = t.toLowerCase();
            const bad = ["learn more","view profile","profile","faculty","staff","department","research"];
            if (bad.includes(low)) return false;
            return true;
          };

          const pickNameProfile = (container) => {
            if (!container) return {name:null, profile_url:null};

            // headings first
            const hs = Array.from(container.querySelectorAll("h1,h2,h3,h4,h5"))
              .map(h => (h.textContent||"").trim())
              .filter(looksLikeName);
            if (hs.length) {
              const a = container.querySelector("h1 a[href],h2 a[href],h3 a[href],h4 a[href],h5 a[href]");
              return { name: hs[0], profile_url: a ? abs(a.getAttribute("href")) : null };
            }

            // then name-like links
            const links = Array.from(container.querySelectorAll("a[href]"))
              .map(a => ({ text:(a.textContent||"").trim(), href: abs(a.getAttribute("href")||"") }))
              .filter(x => x.href && looksLikeName(x.text));
            if (links.length) return { name: links[0].text, profile_url: links[0].href };

            // alt fallback
            const img = container.querySelector("img");
            const alt = img ? (img.alt||"").trim() : "";
            return looksLikeName(alt) ? { name: alt, profile_url: null } : { name: null, profile_url: null };
          };

          const getImgUrl = (img) => {
            let u = img.currentSrc || img.src || "";
            if (!u) {
              const srcset = img.getAttribute("srcset") || "";
              if (srcset) {
                const parts = srcset.split(",").map(s => s.trim().split(" ")[0]).filter(Boolean);
                if (parts.length) u = parts[parts.length - 1];
              }
            }
            return u ? abs(u) : null;
          };

          const out = [];

          // Use images as anchors (most stable)
          const imgs = Array.from(document.querySelectorAll("img"));
          for (const img of imgs) {
            const image_url = getImgUrl(img);
            if (!image_url) continue;

            const low = image_url.toLowerCase();
            if (low.startsWith("data:image")) continue;
            if (badImgKw.some(k => low.includes(k))) continue;

            const w = img.naturalWidth || 0;
            const h = img.naturalHeight || 0;
            if (w && h && (w < 120 || h < 120)) continue;

            // find container around image
            const container = img.closest("article, li, section, div") || img.parentElement;

            // Heuristic: keep containers likely representing a person card
            // If the container has a profile link to samueli.ucla.edu, treat as candidate
            const hasProfileLink = container && container.querySelector('a[href*="samueli.ucla.edu/people/"],a[href*="samueli.ucla.edu/faculty/"],a[href*="samueli.ucla.edu/directory/"]');
            if (!hasProfileLink) continue;

            const { name, profile_url } = pickNameProfile(container);
            out.push({ name, profile_url, image_url });
          }

          // Dedup by image_url (prefer rows with name)
          const map = new Map();
          for (const r of out) {
            if (!r.image_url) continue;
            if (!map.has(r.image_url)) map.set(r.image_url, r);
            else {
              const cur = map.get(r.image_url);
              if (!cur.name && r.name) map.set(r.image_url, r);
            }
          }

          // Filter: require name (to avoid random images)
          return Array.from(map.values()).filter(r => r.name && r.name.trim().length >= 3);
        }
        """)

        print("[info] records found:", len(records))
        if len(records) == 0:
            await context.close()
            await browser.close()
            raise RuntimeError("No records found. Page structure may have changed; need a more specific selector.")

        if MAX_PEOPLE is not None:
            records = records[:MAX_PEOPLE]

        # ---- Download images (optional) + build CSV rows ----
        rows = []
        saved = 0

        for i, r in enumerate(records, 1):
            name = (r.get("name") or f"unknown_{i}").strip()
            profile_url = r.get("profile_url")
            image_url = r.get("image_url")

            local_path = None
            if DOWNLOAD_IMAGES:
                ext = ext_from_url(image_url)
                filename = safe_filename(name) + ext
                local_path = os.path.join(OUT_DIR, filename)

                if not os.path.exists(local_path):
                    try:
                        resp = await context.request.get(image_url, timeout=60000)
                        if resp.ok:
                            ctype = (resp.headers.get("content-type") or "").lower()
                            if "image" in ctype:
                                data = await resp.body()
                                with open(local_path, "wb") as f:
                                    f.write(data)
                                saved += 1
                            else:
                                local_path = None
                        else:
                            local_path = None
                    except Exception:
                        local_path = None

            rows.append({
                "name": name,
                "profile_url": profile_url,
                "image_url": image_url,
                "local_path": local_path
            })

        # ---- Write CSV ----
        with open(CSV_PATH, "w", newline="", encoding="utf-8") as f:
            w = csv.DictWriter(f, fieldnames=["name", "profile_url", "image_url", "local_path"])
            w.writeheader()
            w.writerows(rows)

        await context.close()
        await browser.close()

        print(f"[done] saved images: {saved}/{len(rows)}" if DOWNLOAD_IMAGES else "[done] images download skipped")
        print("[done] CSV:", CSV_PATH)
        print("[done] Images:", OUT_DIR)

        return rows

rows = await scrape_ucla_cs()
print("preview:", rows[:5])

"""UCLA Art (with csv directory)"""

import os, re, csv, time
from urllib.parse import urljoin, urlparse

import requests
from bs4 import BeautifulSoup

START_URL = "https://www.art.ucla.edu/faculty/"

OUT_DIR  = "/content/drive/MyDrive/prof_photos/ucla_art_faculty_photos"
CSV_PATH = "/content/drive/MyDrive/prof_photos/ucla_art_faculty_photos.csv"

# 如果你不在 Colab，把 OUT_DIR/CSV_PATH 改成你电脑路径，比如 "~/Desktop/ucla_art_faculty_photos"
os.makedirs(OUT_DIR, exist_ok=True)
os.makedirs(os.path.dirname(CSV_PATH), exist_ok=True)

HEADERS = {
    "User-Agent": "Mozilla/5.0 (compatible; research-bot/1.0; +contact: your_email@example.com)"
}

def safe_filename(name: str, max_len: int = 140) -> str:
    name = (name or "").strip()
    name = re.sub(r"\s+", " ", name)
    name = re.sub(r"[^a-zA-Z0-9._ -]+", "", name)
    name = name.replace(" ", "_")
    return name[:max_len] if name else "unknown"

def ext_from_content_type(ct: str) -> str:
    ct = (ct or "").lower()
    if "png" in ct: return ".png"
    if "webp" in ct: return ".webp"
    if "jpeg" in ct or "jpg" in ct: return ".jpg"
    return ".jpg"

def unique_path(out_dir: str, base: str, ext: str) -> str:
    path = os.path.join(out_dir, f"{base}{ext}")
    k = 2
    while os.path.exists(path):
        path = os.path.join(out_dir, f"{base}_{k}{ext}")
        k += 1
    return path

session = requests.Session()
session.headers.update(HEADERS)

resp = session.get(START_URL, timeout=30)
resp.raise_for_status()

soup = BeautifulSoup(resp.text, "html.parser")

# --- 1) 抓“像人名的”图片：通常 alt 就是人名 ---
# 排除明显非人像的站点装饰图
bad_alt = {
    "UCLA Department of Art", "UCLA", "Logo", "logo", "Search", "menu"
}
bad_src_kw = ["logo", "icon", "sprite", "banner", "placeholder", "loading", "spinner"]

imgs = []
for img in soup.select("img"):
    alt = (img.get("alt") or "").strip()
    src = img.get("src") or img.get("data-src") or ""
    if not alt or alt in bad_alt:
        continue
    if len(alt) < 3:
        continue
    if not src:
        continue
    full = urljoin(START_URL, src)

    low = full.lower()
    if any(k in low for k in bad_src_kw):
        continue
    if low.startswith("data:image"):
        continue

    imgs.append({"name": alt, "image_url": full})

# 去重（按 image_url）
seen = set()
records = []
for it in imgs:
    if it["image_url"] in seen:
        continue
    seen.add(it["image_url"])
    records.append(it)

print("Found images:", len(records))
if len(records) == 0:
    raise RuntimeError("没抓到任何头像。可能网页结构变了：请把页面中任意一张头像的 <img ...> 片段贴我。")

# --- 2) 下载图片并写 CSV ---
rows = []
saved = 0
for i, r in enumerate(records, 1):
    name = r["name"]
    image_url = r["image_url"]

    try:
        img_resp = session.get(image_url, timeout=30)
        if img_resp.status_code != 200:
            rows.append({"name": name, "image_url": image_url, "local_path": None, "group": "UCLA Art Faculty Page"})
            continue

        ctype = img_resp.headers.get("Content-Type", "")
        if "image" not in (ctype or "").lower():
            rows.append({"name": name, "image_url": image_url, "local_path": None, "group": "UCLA Art Faculty Page"})
            continue

        ext = ext_from_content_type(ctype)
        base = safe_filename(name)
        path = unique_path(OUT_DIR, base, ext)

        with open(path, "wb") as f:
            f.write(img_resp.content)

        saved += 1
        rows.append({"name": name, "image_url": image_url, "local_path": path, "group": "UCLA Art Faculty Page"})

        time.sleep(0.4)  # 礼貌一点
    except Exception:
        rows.append({"name": name, "image_url": image_url, "local_path": None, "group": "UCLA Art Faculty Page"})

with open(CSV_PATH, "w", newline="", encoding="utf-8-sig") as f:
    w = csv.DictWriter(f, fieldnames=["name", "image_url", "local_path", "group"])
    w.writeheader()
    w.writerows(rows)

print(f"Saved images: {saved}/{len(rows)}")
print("Images dir:", OUT_DIR)
print("CSV:", CSV_PATH)

"""week3 model training"""

import pandas as pd
from sklearn.model_selection import KFold, cross_validate
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.dummy import DummyRegressor
from sklearn.linear_model import LinearRegression, Ridge
from sklearn.ensemble import HistGradientBoostingRegressor

df = pd.read_csv("/content/table2_matched_with_table1_last3.csv")

# 1) 删除无效评分行
mask_invalid = (df["avg_rating"] == 0) & (df["avg_difficulty"] == 0) & (df["would_take_again_percent"] == -1)
df = df.loc[~mask_invalid].copy()

# 2) 去掉关键列缺失
df = df.dropna(subset=["avg_rating", "pred_score_raw"]).copy()

# 3) X / y
y = df["avg_rating"].astype(float)

num_features = ["pred_score_raw"]
cat_features = ["dept", "school"]  # 可选：不想用就改成 []

X = df[num_features + cat_features].copy()

# 4) preprocess
numeric_pipe = Pipeline([
    ("imputer", SimpleImputer(strategy="median")),
    ("scaler", StandardScaler()),
])

categorical_pipe = Pipeline([
    ("imputer", SimpleImputer(strategy="most_frequent")),
    ("onehot", OneHotEncoder(handle_unknown="ignore")),
])

preprocess = ColumnTransformer([
    ("num", numeric_pipe, num_features),
    ("cat", categorical_pipe, cat_features),
], remainder="drop")

# 5) baselines
models = {
    "DummyMean": DummyRegressor(strategy="mean"),
    "LinearRegression": LinearRegression(),
    "Ridge(alpha=1.0)": Ridge(alpha=1.0, random_state=0),
    "HistGBR": HistGradientBoostingRegressor(random_state=0),
}

# 6) evaluation
cv = KFold(n_splits=5, shuffle=True, random_state=0)
scoring = {
    "MAE": "neg_mean_absolute_error",
    "RMSE": "neg_root_mean_squared_error",
    "R2": "r2",
}

rows = []
for name, model in models.items():
    pipe = Pipeline([("preprocess", preprocess), ("model", model)])
    out = cross_validate(pipe, X, y, cv=cv, scoring=scoring)
    rows.append({
        "model": name,
        "n": len(y),
        "MAE(mean)": -out["test_MAE"].mean(),
        "RMSE(mean)": -out["test_RMSE"].mean(),
        "R2(mean)": out["test_R2"].mean(),
    })

res = pd.DataFrame(rows).sort_values("MAE(mean)")
print("Invalid removed:", int(mask_invalid.sum()))
print(res.to_string(index=False))